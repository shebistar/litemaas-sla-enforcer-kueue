= *Lab Guide: Agentic AI Agents with Llama Stack Clients*
:stem: latexmath
:icons: font
:toc: left
:source-highlighter: highlight.js
:numbered:


== Introduction

In the next steps we are going to create multiple Agents using the Llama Stack client python package. All agents will be defined within the same Jupyter Notebook.
The agents can use the models and tools defined within the Llama stack config we deployed in the previous part of the lab.

== Notebook Setup

- Access Openshift AI and create a new workbench within a project of your choice using the Jupyter Image with Python 3.12 (You may need to scale the worker MachineSet within your OpenShift cluster to have enough ressources available).

[.bordershadow]
image::agentic_ai_workbench.png[]

- Open the Workbench and create a new notebook

- Install the `llama-stack` client library within a code block:
+
[.console-input]
[source,python]
----
!pip install -qq llama-stack
----

- Import the necessary libraries:
+
[.console-input]
[source,python]
----
import os
from llama_stack_client import LlamaStackClient, Agent, AgentEventLogger
----
+

- Define the connection details for the Llama Stack server. By default, this will use the internal Kubernetes service name.
+
[.console-input]
[source,python]
----
LLAMA_STACK_SERVER_HOST = os.getenv("LLAMA_STACK_SERVER_HOST", "llamastack-with-config-service.llama-stack.svc.cluster.local")
LLAMA_STACK_SERVER_PORT = os.getenv("LLAMA_STACK_SERVER_PORT", "8321")
----

- Your playbook should look like this now:

[.bordershadow]
image::agentic_ai_playbook.png[]


== Agent 1 - Web Search

In this step we will explore the web search tool which enables an agent to fetch information from the web.

Instantiate the client and create an agent. This agent is configured to use the `llama-4-scout-17b-16e-w4a16` model and has access to the web search tool.
[.console-input]
[source,python]
----
client_websearch = LlamaStackClient(base_url=f"http://{LLAMA_STACK_SERVER_HOST}:{LLAMA_STACK_SERVER_PORT}")

agent_websearch = Agent(
    client_websearch,
    model="llama-4-scout-17b-16e-w4a16",
    instructions="You are a helpful assistant.",
    tools=[
        "builtin::websearch",
    ],
    max_infer_iters=5,
    sampling_params={
        "strategy": {"type": "top_p", "temperature": 0.1, "top_p": 0.95},
    },
)
session_websearch = agent_websearch.create_session("monitored_session")
----

Now you can ask the agent questions. This first example uses the web search tool to find the current OpenShift release. The agent then sends the task to the defined model.
The model can decide whether to use any of the available tools or to solve the task with its own capabilities. In case the model decides to use a tool, the model responses to the 
agent with the selected tool call. The agent executes the tool and returns the response back to the LLM. There is a maximum of steps defiend (max_infer_iters=5) that the model can do until it needs to 
sends a final response back to the agent.

[NOTE]
====
If you see any Python import errors when executing a single cell you need to run all cells before the one displaying the error. You can execute all Cells via the 'Run All Cells' Option within the 'Run' menu.
====

[.console-input]
[source,python]
----
response = agent_websearch.create_turn(
    messages=[{"role": "user", "content": "Whats the current Red Hat OpenShift release?"}],
    session_id=session_websearch,
)

for log in AgentEventLogger().log(response):
    log.print()
----

[NOTE]
====
In case you see a wrong answer you can rerun the command via the 'Run Selected Cell' within the 'Run' menu.
====

[.bordershadow]
image::agentic_ai_agent_one.png[]


== Agent 2 - OpenShift MCP Server

The second agent we create has access to the OpenShift MCP Server to retrieve OpenShift API information. The agent uses the 'llama-3-2-3b' model.
[.console-input]
[source,python]
----
client_mcp_ocp = LlamaStackClient(base_url=f"http://{LLAMA_STACK_SERVER_HOST}:{LLAMA_STACK_SERVER_PORT}")

agent_mcp_ocp = Agent(
    client_mcp_ocp,
    model="llama-3-2-3b",
    instructions="You are a helpful assistant",
    tools=[
        "mcp::openshift"
    ],
    max_infer_iters=5,
    sampling_params={
        "strategy": {"type": "top_p", "temperature": 0.1, "top_p": 0.95},
        "max_tokens": 8000,
    },
)
session_mcp_ocp = agent_mcp_ocp.create_session("monitored_session")
----

It's now possible to ask the agent questions about the OpenShift cluster and the agent is able to receive data via the MCP server:
[.console-input]
[source,python]
----
response = agent_mcp_ocp.create_turn(
    messages=[{"role": "user", "content": "What pods are running in the llama-stack namespace?"}],
    session_id=session_mcp_ocp,
)

for log in AgentEventLogger().log(response):
    log.print()
----

[.bordershadow]
image::agentic_ai_agent_two.png[]


== Agent 3 - Websearch & MCP

The third agent we create has access to the mcp server as well as the web search tool.
[.console-input]
[source,python]
----
client_mutli = LlamaStackClient(base_url=f"http://{LLAMA_STACK_SERVER_HOST}:{LLAMA_STACK_SERVER_PORT}")

agent_multi = Agent(
    client_mutli,
    model="llama-4-scout-17b-16e-w4a16",
    instructions="You are an assistant helping to debug OpenShift cluster issues.",
    tools=[
        "mcp::openshift",
        "builtin::websearch"
    ],
    max_infer_iters=5,
    sampling_params={
        "strategy": {"type": "top_p", "temperature": 0.1, "top_p": 0.95},
        "max_tokens": 8000,
    },
)
session_multi = agent_multi.create_session("monitored_session")
----

Let's apply a pod that will fail if a specific environment variable is not set to our cluster as an investiation target for the agent:

[.console-input]
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: fail-crash-loop
  namespace: default
spec:
  containers:
    - name: alpine
      image: alpine:3.19
      command:
        - sh
        - -c
        - |
          if [ -z "$IMPORTANT_MESSAGE" ]; then
            echo "ERROR: IMPORTANT_MESSAGE is not set. Exiting."
            exit 1
          else
            echo "IMPORTANT_MESSAGE is set to: $IMPORTANT_MESSAGE"
          fi
----

Apply the `Pod` using `oc apply -f broken-pod.yaml`.

[.bordershadow]
image::agentic_ai_failing_pod.png[]


Its now possible to ask the agent questions about the OpenShift cluster and its able to receive data via the mcp server:
[.console-input]
[source,python]
----
messages=[{"role": "user", "content": "Search for pods having problems in the default namespace using the OpenShift mcp."},
          {"role": "user", "content": "Investiage the failing pod and suggest a fix"},
          {"role": "user", "content": "Look up relevant troubleshooting information from the web."}
         ]
for message in messages:
    print("\n"+"="*50)
    print(f"Processing user query: {message}")
    print("="*50)
    response = agent_multi.create_turn(
      messages=[message],
        session_id=session_multi,
    )
    
    for log in AgentEventLogger().log(response):
        log.print()
----


== Next steps:

If there is enough time within the session you can adapt the available agent inputs (edit the content parts of the messages) and for example explore the following:

- Explore the web search. Ask for specific information about a recent event (sport, concerts etc.).
- Explore the OpenShift MCP server with it tools.
- See the Llama Stack storage in action. Add an information to the input array and ask for it in the next entry.
