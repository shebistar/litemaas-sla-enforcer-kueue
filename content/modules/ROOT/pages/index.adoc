= Taming GPU Chaos: Mastering AI Quotas with Kueue
:author: Red Hat
:revnumber: 1.0
:revdate: {docdate}
:toc: left
:toclevels: 3
:sectnums:

:description: A hands-on lab demonstrating how to enforce AI/ML SLAs using Red Hat OpenShift AI and the Kueue Operator.
:keywords: kueue, openshift, ai, ml, gpu, quota, preemption, ray, red hat

[abstract]
This lab provides a hands-on demonstration of how to centralize, monetize, and control expensive AI compute infrastructure (like GPUs) using the Red Hat build of Kueue Operator on Red Hat OpenShift AI (RHOAI). You will simulate a Models-as-a-Service (MaaS) provider who offers different service tiers to customers, and then use Kueue's powerful preemption capabilities to enforce a Service Level Agreement (SLA) in real-time.

== 1. Overview and Business Value

Organizations often struggle with fragmented AI deployments, leading to idle GPUs and soaring operational costs. A MaaS provider centralizes this infrastructure to optimize hardware utilization, but requires a strong technical mechanism to enforce control and guarantee service levels for different pricing tiers. This lab demonstrates that mechanism.

.Business Goals and Technical Solutions
|===
| MaaS Challenge (Business Goal) | Kueue Solution (Lab Action)

| *Monetization & SLAs:*
Offering guaranteed vs. best-effort service levels.
| *Pricing Tiers:*
Define segregated `ClusterQueues` with `nominalQuota` to guarantee capacity for premium users (*Reserved Tier*).

| *SLA Enforcement & Control:*
Guaranteeing mission-critical workloads always run.
| *Preemption:*
Use a high-priority job to automatically evict a running low-priority job when reserved resources are required.

| *Efficient Utilization:*
Allowing development teams to borrow idle resources safely.
| *Cohort Sharing:*
Enable teams to draw from a shared resource pool for CPU/Memory when available.
|===

[Image of a cloud computing architecture diagram]

== 2. Lab Architecture

This lab simulates a multi-tenant environment on OpenShift using several key components:

* *Red Hat OpenShift AI (RHOAI):* Provides the foundational AI/ML platform and the Ray Operator for running distributed workloads.
* *Red Hat build of Kueue Operator:* The core component for queueing, quota management, and SLA enforcement.
* *RayCluster:* A distributed computing workload, representing a typical AI/ML job that consumes significant resources.

We will configure two service tiers:

1.  *Reserved Tier (`team-a`):* Represents a premium customer with a guaranteed quota for GPUs and CPU/Memory. Their workloads run at a high priority.
2.  *On-Demand Tier (`team-b`):* Represents a standard customer that can borrow idle CPU/Memory from a shared pool on a best-effort basis. Their workloads run at a low priority.

== 3. Prerequisites and Setup

Before executing the lab steps, you must ensure the environment is correctly configured.

=== 3.1. Required Operators

Ensure the following Operators are installed and running on your OpenShift cluster:

* Red Hat OpenShift AI Operator
* Red Hat build of Kueue Operator

=== 3.2. Enable the Ray Component in OpenShift AI

The `RayCluster` resources used in this lab require the Ray Operator to be active. This is managed by RHOAI.

. First, check if a `DataScienceCluster` resource already exists:
+
[.console-input]
[source,bash]
----
oc get datasciencecluster default-dsc -n redhat-ods-applications
----

. If the command above returns a `NotFound` error, you must create it. The following command will **create** the `DataScienceCluster` with Ray enabled, or **update** it if it already exists.

+
[.console-input]
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: datasciencecluster.opendatahub.io/v1
kind: DataScienceCluster
metadata:
  name: default-dsc
  namespace: redhat-ods-applications
spec:
  components:
    ray:
      managementState: Managed
EOF
----

. After running the command, verify that the Ray Operator pod starts successfully. Wait for the `kuberay-operator-...` pod to enter the 'Running' state before proceeding.
+
[.console-input]
[source,bash]
----
oc get pods -n redhat-ods-applications -w
----

== 4. Lab Execution: SLA Enforcement

This sequence demonstrates Kueue's ability to enforce quotas and SLAs through live preemption.

=== 4.1. Step 1: Configure MaaS Tiers and Quotas

Apply the `setup.yaml` file. This single file creates all the necessary resources for the demo: namespaces, priority classes, resource flavors, and the ClusterQueue/LocalQueue topology that defines our service tiers.

[source,bash]
[.console-input]
----
# Apply namespaces, CQs, LQs, ResourceFlavors, and Priority Classes
oc apply -f setup.yaml
----

Verify that the ClusterQueues are active:
[.console-input]
[source,bash]
----
oc get cq
# NAME        COHORT   STRATEGY         PENDING WORKLOADS   ADMITTED WORKLOADS
# shared-cq   team-ab  BestEffortFIFO   0                   0
# team-a-cq   team-ab  BestEffortFIFO   0                   0
# team-b-cq   team-ab  BestEffortFIFO   0                   0
----

=== 4.2. Step 2: Deploy Low-Priority (On-Demand) Workload

Simulate the On-Demand customer (`team-b`) deploying a non-critical workload. This job will be admitted by Kueue and consume resources from the shared pool.

[.console-input]
[source,bash]
----
# Deploy Team B's workload (raycluster-dev)
oc apply -f workload-b-low-priority.yaml
----

Wait for the pods to start running and the workload to be admitted.

[.console-input]
[source,bash]
----
oc get pods -n team-b -w
oc get workload -n team-b # Verify ADMITTED is True
----

=== 4.3. Step 3: Demonstrate Quota Enforcement (Optional)

To prove that the quota is working, deploy a second, competing low-priority workload. Because the first job has consumed all available shared resources, this job will be queued by Kueue and will remain `Pending`.

[.console-input]
[source,bash]
----
# Deploy the competing workload
oc apply -f workload-b-competing.yaml
----
[.console-input]
[source,bash]
----
oc get workload -n team-b
# NAME                                 JOB TYPE     JOB NAME                  LOCAL QUEUE   CLUSTER QUEUE   STATUS    AGE
# raycluster-raycluster-dev...         RayCluster   raycluster-dev            lq-team-b     team-b-cq       Admitted  96s
# raycluster-raycluster-dev-comp...    RayCluster   raycluster-dev-competing  lq-team-b     team-b-cq       Pending   15s
----

=== 4.4. Step 4: Deploy High-Priority (Reserved) Workload

Simulate the Reserved MaaS customer (`team-a`) submitting a critical workload. Because its guaranteed resources are currently being used by the low-priority job from `team-b`, Kueue must enforce the SLA.

[source,bash]
[.console-input]
----
# Deploy Team A's workload (raycluster-prod)
oc apply -f workload-a-high-priority.yaml
----

=== 4.5. Step 5: Observe Live Preemption

This is the eye-catching moment of the demo. Monitor the workloads across all namespaces.

[.console-input]
[source,bash]
----
oc get workload -A -w
----

You will observe the following in real-time:
. The `raycluster-dev` (Team B) workload will have its `ADMITTED` status switch from `True` to `False`.
. The `raycluster-prod` (Team A) workload will have its `ADMITTED` status switch to `True`.
. The pods in the `team-b` namespace will begin terminating as they are evicted.

=== 4.6. Step 6: Verify Policy Enforcement

Describe the preempted workload to see the explicit audit trail left by Kueue.

[.console-input]
[source,bash]
----
oc describe workload -n team-b raycluster-raycluster-dev-....
----

Look for the *Events* section at the bottom. You will see a message confirming that the workload was **Evicted** to make room for a higher-priority workload, proving the SLA was automatically enforced.

== 5. Cleanup

To remove all resources created during this lab, execute the following commands.

[.console-input]
[source,bash]
----
# 1. Delete the namespaces and all contained resources
oc delete ns team-a team-b

# 2. Delete the cluster-scoped Kueue objects
oc delete clusterqueue --all
oc delete resourceflavor --all
oc delete workloadpriorityclass --all
oc delete priorityclass prod-priority dev-priority
----
