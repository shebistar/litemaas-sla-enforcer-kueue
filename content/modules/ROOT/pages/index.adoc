= Taming GPU Chaos: Mastering AI Quotas with Kueue
:author: Red Hat
:revnumber: 1.0
:revdate: {docdate}
:toc: left
:toclevels: 3
:sectnums:

:description: A hands-on lab demonstrating how to enforce AI/ML SLAs using Red Hat OpenShift AI and the Kueue Operator.
:keywords: kueue, openshift, ai, ml, gpu, quota, preemption, ray, red hat

[abstract]
This lab provides a hands-on demonstration of how to centralize, monetize, and control expensive AI compute infrastructure (like GPUs) using the Red Hat build of Kueue Operator on Red Hat OpenShift AI (RHOAI). You will simulate a Models-as-a-Service (MaaS) provider who offers different service tiers to customers, and then use Kueue's powerful preemption capabilities to enforce a Service Level Agreement (SLA) in real-time.

== 1. Overview and Business Value

Organizations often struggle with fragmented AI deployments, leading to idle GPUs and soaring operational costs. A MaaS provider centralizes this infrastructure to optimize hardware utilization, but requires a strong technical mechanism to enforce control and guarantee service levels for different pricing tiers. This lab demonstrates that mechanism.

.Business Goals and Technical Solutions
|===
| MaaS Challenge (Business Goal) | Kueue Solution (Lab Action)

| *Monetization & SLAs:*
Offering guaranteed vs. best-effort service levels.
| *Pricing Tiers:*
Define segregated `ClusterQueues` with `nominalQuota` to guarantee capacity for premium users (*Reserved Tier*).

| *SLA Enforcement & Control:*
Guaranteeing mission-critical workloads always run.
| *Preemption:*
Use a high-priority job to automatically evict a running low-priority job when reserved resources are required.

| *Efficient Utilization:*
Allowing development teams to borrow idle resources safely.
| *Cohort Sharing:*
Enable teams to draw from a shared resource pool for CPU/Memory when available.
|===

== 2. Lab Architecture

This lab simulates a multi-tenant environment on OpenShift using several key components:

* *Red Hat OpenShift AI (RHOAI):* Provides the foundational AI/ML platform and the Ray Operator for running distributed workloads.
* *Red Hat build of Kueue Operator:* The core component for queueing, quota management, and SLA enforcement.
* *RayCluster:* A distributed computing workload, representing a typical AI/ML job that consumes significant resources.

We will configure two service tiers:

1.  *Reserved Tier (`team-a`):* Represents a premium customer with a guaranteed quota for GPUs and CPU/Memory. Their workloads run at a high priority.
2.  *On-Demand Tier (`team-b`):* Represents a standard customer that can borrow idle CPU/Memory from a shared pool on a best-effort basis. Their workloads run at a low priority.

== 3. Prerequisites and Setup

Before executing the lab steps, you must ensure the environment is correctly configured.

=== 3.1. Deploy the Lab Environment from the Demo Catalog

The very first step is to provision the correct environment from the Red Hat Demo Platform. This lab is designed to run on a specific configuration that includes OpenShift with pre-configured NVIDIA GPUs.

*Navigate to the demo catalog and deploy an instance of the **RHOAI on OCP on AWS with NVIDIA GPUs** demo.*

link:{https://catalog.demo.redhat.com/catalog?search=RHOAI+on+OCP+on+AWS+with+NVIDIA+GPUs&item=babylon-catalog-prod%2Fsandboxes-gpte.ocp4-demo-rhods-nvidia-gpu-aws.prod}[Click here to access the demo in the catalog].

Once the environment is running, you can log in to the OpenShift cluster and proceed with the next steps.

=== 3.2. Required Operators

Ensure the following Operators are installed and running on your OpenShift cluster. The demo environment from the catalog should have these pre-installed.

* Red Hat OpenShift AI Operator
* Red Hat build of Kueue Operator
* NVIDIA GPU Operator

=== 3.3. Enable the Ray Component in OpenShift AI

The `RayCluster` resources used in this lab require the Ray Operator to be active. This is managed by RHOAI.

. First, check if a `DataScienceCluster` resource already exists:
+
[.console-input]
[source,bash]
----
oc get datasciencecluster default-dsc -n redhat-ods-applications
----

. If the command above returns a `NotFound` error, you must create it. The following command will **create** the `DataScienceCluster` with Ray enabled, or **update** it if it already exists.

+
[.console-input]
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: datasciencecluster.opendahub.io/v1
kind: DataScienceCluster
metadata:
  name: default-dsc
  namespace: redhat-ods-applications
spec:
  components:
    ray:
      managementState: Managed
EOF
----

. After running the command, verify that the Ray Operator pod starts successfully. Wait for the `kuberay-operator-...` pod to enter the 'Running' state before proceeding.
+
[.console-input]
[source,bash]
----
oc get pods -n redhat-ods-applications -w
----

== 4. Lab Workflow: The SLA Preemption Scenario

This lab demonstrates a complete lifecycle of quota management and SLA enforcement. After applying the `setup.yaml` file to establish the service tiers, the demonstration unfolds in four key phases:

. *Phase 0: Configure GPU Time-Slicing*
To ensure the lab can demonstrate multi-tenancy on limited GPU hardware, the first step is to enable the NVIDIA GPU Operator's Time-Slicing feature. This powerful capability partitions a single physical GPU into multiple smaller, virtual GPUs (vGPUs) that can be shared by different pods. For this lab, the execution steps will guide you through creating a configuration that slices each physical GPU into two virtual replicas. This allows more workloads to run concurrently, making the subsequent quota enforcement and preemption scenarios more impactful and easier to observe.

. *Phase 1: On-Demand Usage & Quota Enforcement*
The low-priority "On-Demand" user (`team-b`) submits their first workload (`workload-b-low-priority.yaml`). It is admitted by Kueue and begins consuming the shared resources. They then submit a second, competing workload (`workload-b-competing.yaml`). Because the first job has exhausted the shared quota, Kueue enforces the limit and this second workload remains `Pending` in the queue. This demonstrates basic resource control.

. *Phase 2: SLA Invocation*
The high-priority "Reserved" user (`team-a`) submits their critical workload (`workload-a-high-priority.yaml`). This workload has a right to its guaranteed resources, but those resources (specifically CPU/Memory) are currently being used by `team-b`'s running job.

. *Phase 3: Live Preemption*
Kueue's preemption logic activates. It identifies that `team-a`'s workload has a higher priority and a claim to the resources. Kueue automatically revokes the admission of `team-b`'s workload, which causes its pods to be terminated. As soon as the resources are freed, Kueue admits `team-a`'s workload, which then starts running. The SLA has been enforced automatically, without any manual intervention.

