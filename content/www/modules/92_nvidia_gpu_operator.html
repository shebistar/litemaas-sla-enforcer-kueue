<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Lab Guide: Configuring NVIDIA GPU Time-Slicing on OpenShift :: Platform Foundation Bootcamp - RHOAI</title>
    <link rel="canonical" href="https://redhat-ai-services.github.io/rhoai-platform-foundation-bootcamp-instructions/modules/92_nvidia_gpu_operator.html">
    <meta name="generator" content="Antora 3.1.14">
<link rel="stylesheet" href="../_/css/site.css"><link rel="stylesheet" href="../_/css/site-extra.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<meta name="robots" content="all">
<link rel="icon" href="../_/img/favicon.ico" type="image/x-icon">
    <link rel="icon" href="https://demo.redhat.com/images/favicon.ico" type="image/x-icon">
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar"  style="background-color: #131313 !important;">
    <div class="navbar-brand">
      <div style="display: flex; flex-direction:row; padding: 12px 32px; gap: 16px;">
     </div>
      <div class="navbar-item site-title" style="color: #fff !important;flex: 1;">Platform Foundation Bootcamp - RHOAI</div>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="modules" data-version="">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title" style="display: none;"><a href="index.html" class=" query-params-link">Navigation</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="01_welcome.html">Welcome and Introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="05_environment_provisioning.html">Environment Provisioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">AI-Accelerator</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="20_ai-accelerator_review.html">Project Overview</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="07_installation.html">Bootstrap Installation</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="30_gitops_env_setup_dev_prod.html">Setup Dev &amp; Prod Environments</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">RHOAI Administration</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="32_dashboard_configuration.html">RHOAI Dashboard Configuration</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Notebooks</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="31_custom_notebook.html">Custom Notebook</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="09_remote_connect_notebook.html">Connect to Workbench Kernel</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Train, Store (S3), Deploy</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="33_model_training_car.html">Model Training</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="34_using_s3_storage.html">Using S3 Storage</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="36_deploy_model.html">Deploy Model</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Data Science Pipelines</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="40_setup_pipeline_server.html">Setup Pipeline Server</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="41_introduction_to_kfp_pipelines.html">KFP Pipelines</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="kfp_elyra_differences.html">Comparison between Elyra and KFP</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="build_custom_runtime_image.html">Custom Runtime Image</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="introduction_to_elyra_pipelines.html">Elyra Pipelines</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="42_working_with_pipelines.html">Working with Pipelines</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="43_custom_runtime_image.html">Advanced Pipeline Customization</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Distributed Training</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="50_distributed_training.html">Ray Cluster</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Large Language Model [LLM]</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="60_llm_explore.html">Explore LLMs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="70_rag_llm.html">RAG with LLM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="composer_ai.html">Composer AI</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Monitoring Data Science Models</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="80_trustyai_overview.html">TrustyAI Overview</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="81_llm_evaluation.html">Evaluating Large Language Models</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Disconnected Environment</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="disconnected_install.html">RHOAI on Disconnected Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">GPU as a Service</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="90_environment_provisioning.html">Provisioning a GPU Environment with NVIDIA A10G Tensor Core GPU</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="91_gpu_as_a_service_intro.html">Introduction: GPU as a Service with GPU slicing and Kueue</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="92_nvidia_gpu_operator.html">Configuring NVIDIA GPU Time-Slicing on OpenShift</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="93_kueue_setup.html">Red Hat build of Kueue Operator Setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="94_kueue_gpu_pricing_tier.html">Implementing GPU Pricing Tiers with Kueue</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="95_kueue_fair_sharing.html">Advanced GPU Quota Management and Preemption with Kueue</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Model-as-a-Service</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="100_maas_intro.html">Introduction</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="101_maas_bootstrap.html">Environment Bootstrap</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="102_maas_as_developer.html">Using MaaS as Developer</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="103_maas_as_platform_engineer.html">Configure a new model</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="104_maas_monitor.html">Monitor usage</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Agentic AI with Llama Stack</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="97_agentic_ai_llama_stack_introduction.html">Introduction &amp; Setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="98_agentic_ai_llama_stack_notebook_agents.html">Agentic AI Agents with Llama Stack Clients</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="99_agentic_ai_llama_stack_playground.html">Llama Stack Playground</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="#99_useful_tips.adoc">Useful Tips</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="#97_syncing_fork.adoc">Syncing Forked Project</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
  </div>
  <ul class="components">
    <li class="component is-current">
      <span class="title">Navigation</span>
      <ul class="versions">
        <li class="version is-current">
          <a href="index.html">default</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="01_welcome.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li>GPU as a Service</li>
    <li><a href="92_nvidia_gpu_operator.html">Configuring NVIDIA GPU Time-Slicing on OpenShift</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<article class="doc">
<h1 class="page">Lab Guide: Configuring NVIDIA GPU Time-Slicing on OpenShift</h1>
<div id="toc" class="toc">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_introduction">1. Introduction</a></li>
<li><a href="#_prerequisites">2. Prerequisites</a>
<ul class="sectlevel2">
<li><a href="#_node_feature_discovery_nfd_operator">2.1. Node Feature Discovery (NFD) Operator</a></li>
<li><a href="#_nvidia_gpu_operator">2.2. NVIDIA GPU Operator</a></li>
</ul>
</li>
<li><a href="#_verify_nvidia_gpu_operator_and_node_status">3. Verify NVIDIA GPU Operator and Node Status</a></li>
<li><a href="#_configure_the_nvidia_gpu_operator">4. Configure the NVIDIA GPU Operator</a>
<ul class="sectlevel2">
<li><a href="#_theoretical_mig_configuration_example">4.1. Theoretical: MIG Configuration Example</a></li>
<li><a href="#_practical_time_slicing_configuration">4.2. Practical: Time-Slicing Configuration</a></li>
</ul>
</li>
<li><a href="#_configure_hardware_profile_in_openshift_ai">5. Configure Hardware Profile in OpenShift AI</a>
<ul class="sectlevel2">
<li><a href="#_create_hardware_profiles_in_rhoai">5.1. Create Hardware Profiles in RHOAI</a></li>
</ul>
</li>
<li><a href="#_verify_the_configuration">6. Verify the Configuration</a>
<ul class="sectlevel2">
<li><a href="#_create_models">6.1. Create Models</a></li>
<li><a href="#_inspect_the_resource_requests">6.2. Inspect the resource requests</a></li>
</ul>
</li>
<li><a href="#_references">References</a></li>
</ul>
</div>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>This lab guide uses the <a href="https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/introduction.html">NVIDIA GPU Operator on OpenShift</a> and demonstrates the configuration needed to enable GPU <strong>time-slicing</strong>. This approach improves GPU utilization by allowing multiple workloads to share a single GPU.</p>
</div>
<div class="paragraph">
<p>The final configuration will use <strong>time-slicing</strong> because the <strong>NVIDIA A10G Tensor Core GPU</strong>, which is part of the provisioned <a href="https://aws.amazon.com/ec2/instance-types/g5/">Amazon EC2 G5 Instance</a>, does <strong>not</strong> support Multi-Instance GPU (MIG).
However, as <strong>MIG plays a crucial role in production environments</strong>, this guide also includes a theoretical section explaining how a MIG configuration would be implemented.</p>
</div>
<div class="paragraph">
<p>In this lab, each worker node will be provisioned with a single NVIDIA A10G Tensor Core GPU. The created time slices will then be used to simulate a scenario of <strong>fair resource sharing</strong> with Kueue.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">Time-Slicing vs. MIG for Model Serving</div>
<div class="paragraph">
<p>NVIDIA time-slicing is a valuable strategy for serving models when you need to maximize GPU utilization on non-MIG capable hardware, have mixed or bursty workloads, and prioritize cost efficiency in a trusted environment. For newer GPUs and scenarios demanding strong performance isolation and predictable resource allocation, NVIDIA MIG is the preferred choice.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_introduction"><a class="anchor" href="#_introduction"></a>1. Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Installing the NVIDIA GPU Operator on OpenShift is a prerequisite for utilizing NVIDIA GPUs. The operator&#8217;s installation relies on the Node Feature Discovery (NFD) Operator, which must be installed first. The NFD Operator inspects node hardware and applies labels that the NVIDIA GPU Operator uses to identify and configure GPU-enabled nodes correctly.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_prerequisites"><a class="anchor" href="#_prerequisites"></a>2. Prerequisites</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>OpenShift AI Operator:</strong> Ensure the OpenShift AI Operator is installed on your cluster.</p>
</li>
<li>
<p><strong>GPU Worker Node:</strong> You need at least one worker node with an NVIDIA A10G GPU. On AWS, a <code>g5.2xlarge</code> instance is suitable.</p>
</li>
<li>
<p><strong>GPU Node Taint:</strong> The GPU node must be tainted to ensure only GPU-tolerant workloads are scheduled on it.</p>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>To check the current taints on your GPU nodes, use the following commands:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get nodes --selector=nvidia.com/gpu.present=true</code></pre>
</div>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc describe node *&lt;your-gpu-node-name&gt;* | grep "Taints:"</code></pre>
</div>
</div>
<div class="paragraph">
<p>This taint was likely applied during the bootstrap process in the previous lab. If you need to re-apply it, use the following command:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc adm taint nodes *&lt;your-gpu-node-name&gt;* nvidia.com/gpu=Exists:NoSchedule --overwrite</code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_node_feature_discovery_nfd_operator"><a class="anchor" href="#_node_feature_discovery_nfd_operator"></a>2.1. Node Feature Discovery (NFD) Operator</h3>
<div class="paragraph">
<p>OpenShift (and Kubernetes in general) is designed to be hardware-agnostic. It doesn&#8217;t inherently know what specific hardware components (like NVIDIA GPUs) are present on its nodes. The NFD Operator fills this gap.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Standardized labeling: Once NFD discovers a specific hardware feature, like an NVIDIA GPU, it applies a standardized Kubernetes label to that node. For NVIDIA GPUs, the most common label is <code>feature.node.kubernetes.io/pci-10de.present=true</code>, where:</p>
<div class="ulist">
<ul>
<li>
<p><code>feature.node.kubernetes.io/</code> - a standard prefix for NFD-generated labels.</p>
</li>
<li>
<p><code>pci-10de</code> - the PCI vendor ID for NVIDIA Corporation ("10de"). This uniquely identifies NVIDIA hardware.</p>
</li>
<li>
<p><code>.present=true</code> - indicates that a device with this PCI ID is present on the node.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_nvidia_gpu_operator"><a class="anchor" href="#_nvidia_gpu_operator"></a>2.2. NVIDIA GPU Operator</h3>
<div class="paragraph">
<p>The NVIDIA GPU Operator is designed to be intelligent and efficient. It doesn&#8217;t want to deploy its heavy components (like the NVIDIA driver daemonset, container toolkit, device plugin) on every node in your cluster, especially if most of your nodes don&#8217;t have GPUs.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The GPU Operator uses these NFD labels as a selector. It deploys its components only to nodes that have the <code>feature.node.kubernetes.io/pci-10de.present=true</code> label. This ensures that resources are not wasted on non-GPU nodes.</p>
</li>
<li>
<p>Beyond the GPU Operator&#8217;s internal logic, these labels are fundamental for Kubernetes' scheduler. When a user defines a Pod that requires GPU resources (e.g., by specifying <code>resources.limits.nvidia.com/gpu: 1</code>), the Kubernetes scheduler looks for nodes that have the necessary GPU capacity. The labels provided by NFD are crucial for this matching process. Without them, Kubernetes wouldn&#8217;t know which nodes are "GPU-enabled."</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>As stated in the official <a href="#documentation">[documentation]</a>:</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>In addition, the worker nodes can host one or more GPUs, but they must be of the same type. For example, a node can have two NVIDIA A100 GPUs, but a node with one A100 GPU and one T4 GPU is not supported. The NVIDIA Device Plugin for Kubernetes does not support mixing different GPU models on the same node.</p>
</div>
</blockquote>
<div class="attribution">
&#8212; Red Hat<br>
<cite>OpenShift Documentation, Version 4.19</cite>
</div>
</div>
<div class="paragraph">
<p>Multiple Nodes with Different GPU Types: This is the most common and recommended approach. You dedicate individual worker nodes to a specific GPU model. For example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Node 1:</strong> Two NVIDIA A100 GPUs</p>
</li>
<li>
<p><strong>Node 2:</strong> Four NVIDIA T4 GPUs</p>
</li>
<li>
<p><strong>Node 3:</strong> No GPUs (CPU-only)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Your cluster can still have a mix of these node types.
The maximum number of GPUs per Node is limited by the number of <a href="https://www.hp.com/us-en/shop/tech-takes/what-are-pcie-slots-pc">PCI slots</a> within the Mainboard of the Node.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>DO NOT DEPLOY THE NVIDIA NETWORK OPERATOR IN THIS LAB!</strong></p>
</div>
<details>
<summary class="title">NVIDIA Network Operator (For Information Only)</summary>
<div class="content">
<h3 id="_nvidia_network_operator" class="discrete">NVIDIA Network Operator</h3>
<div class="paragraph">
<p>The NVIDIA Network Operator for OpenShift is a specialized Kubernetes Operator designed to simplify the deployment and management of high-performance networking capabilities provided by NVIDIA (formerly Mellanox) in Red Hat OpenShift clusters. It&#8217;s particularly crucial for workloads that demand high-throughput and low-latency communication, such as AI/ML, HPC (High-Performance Computing), and certain telco applications (like vRAN).
The NVIDIA Network Operator works in close conjunction with the NVIDIA GPU Operator. While the GPU Operator focuses on provisioning and managing NVIDIA GPUs (drivers, container runtime, device plugins), the Network Operator handles the networking components that enable:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>RDMA (Remote Direct Memory Access):</strong> Allows direct memory access from the memory of one computer to that of another without involving the operating system, significantly reducing latency and CPU overhead for data transfers.</p>
</li>
<li>
<p><strong>GPUDirect RDMA:</strong> An NVIDIA technology that enables a direct path for data exchange between NVIDIA GPUs and network adapters (like ConnectX series) with RDMA capabilities. This bypasses the CPU and system memory, leading to extremely low-latency, high-bandwidth data transfers, which is critical for distributed deep learning and HPC.</p>
</li>
<li>
<p><strong>SR-IOV (Single Root I/O Virtualization):</strong> Allows a single physical network adapter to be shared by multiple virtual machines or containers as if they had dedicated hardware, improving network performance and reducing overhead.</p>
</li>
<li>
<p><strong>High-speed secondary networks:</strong> Providing dedicated network interfaces for application traffic, separate from the OpenShift cluster&#8217;s primary network. This is crucial for performance-sensitive workloads.</p>
</li>
</ul>
</div>
</div>
</details>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_verify_nvidia_gpu_operator_and_node_status"><a class="anchor" href="#_verify_nvidia_gpu_operator_and_node_status"></a>3. Verify NVIDIA GPU Operator and Node Status</h2>
<div class="sectionbody">
<div class="paragraph">
<p>First, verify that the GPU Operator is running and that the GPUs are recognized.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Get the name of a <code>nvidia-driver-daemonset</code> pod.</p>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get pod -o wide -l openshift.driver-toolkit=true -n nvidia-gpu-operator</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example Output</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-text hljs" data-lang="text">NAME                                           READY   STATUS    RESTARTS   AGE   IP           NODE                                      NOMINATED NODE
nvidia-driver-daemonset-abcdef-12345           2/2     Running   0          19m   10.130.0.9   ip-10-0-61-182.us-east-2.compute.internal   &lt;none&gt;
nvidia-driver-daemonset-abcdef-67890           2/2     Running   0          19m   10.129.0.14  ip-10-0-45-75.us-east-2.compute.internal    &lt;none&gt;</code></pre>
</div>
</div>
</li>
<li>
<p>Execute the <code>nvidia-smi</code> command inside one of the pods to inspect the GPU.</p>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc exec -it -n nvidia-gpu-operator &lt;name-of-driver-daemonset-pod&gt; -- nvidia-smi</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">Example Output</div>
The date and driver versions in this output are examples and may differ from your environment.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre>+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |
|  0%   26C    P8             24W /  300W |       0MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+</pre>
</div>
</div>
<div class="paragraph">
<p>Since there are two GPU-enabled nodes, their configurations could be different. It&#8217;s worth checking both if you encounter issues.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Short version:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc exec -it -n nvidia-gpu-operator $(oc get pod -o wide -l openshift.driver-toolkit=true -o jsonpath="{.items[0].metadata.name}" -n nvidia-gpu-operator) -- nvidia-smi</code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_configure_the_nvidia_gpu_operator"><a class="anchor" href="#_configure_the_nvidia_gpu_operator"></a>4. Configure the NVIDIA GPU Operator</h2>
<div class="sectionbody">
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
The GPUs available in this lab are NVIDIA A10G, which do <strong>not</strong> support MIG. Therefore, we will use <strong>time-slicing</strong>.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_theoretical_mig_configuration_example"><a class="anchor" href="#_theoretical_mig_configuration_example"></a>4.1. Theoretical: MIG Configuration Example</h3>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
This section is for informational purposes only to show how MIG would be configured in a production environment with compatible hardware (e.g., A100 or H100). <strong>Do not apply these configurations in this lab.</strong>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>NVIDIA&#8217;s Multi-Instance GPU (MIG) slicing is a powerful feature that allows you to partition a single compatible NVIDIA GPU (such as the <code>A100</code> or <code>H100</code>) into several smaller, fully isolated, and independent GPU instances. This offers significant advantages, especially in multi-tenant or diverse workload environments. The <a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html#example-custom-mig-configuration-during-installation">Custom MIG Configuration During Installation</a> documentation explains further configuration possibilities.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Hardware-Level Isolation and Security</p>
</li>
<li>
<p>Predictable Performance and Quality of Service (QoS)</p>
</li>
<li>
<p>Maximized GPU Utilization and Cost Efficiency</p>
</li>
<li>
<p>Fine-Grained Resource Allocation and Flexibility</p>
</li>
<li>
<p>Simplified Management in Containerized Environments (e.g., Kubernetes)</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_configmap_for_mig"><a class="anchor" href="#_configmap_for_mig"></a>4.1.1. ConfigMap for MIG</h4>
<div class="paragraph">
<p>Create a <code>ConfigMap</code> to specify the MIG configuration:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Create a <code>yaml</code> file to define how you want to slice your GPUs.</p>
</li>
<li>
<p>This <code>ConfigMap</code> ⚡ <strong>must be named</strong> <code>custom-mig-config</code> and <strong>reside in</strong> the <code>nvidia-gpu-operator</code> namespace ⚡.</p>
</li>
<li>
<p>You can define the mig devices in a custom config. Always make sure to use a <a href="https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#a100-mig-profiles">supported configuration</a>.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-mig-config
data:
  config.yaml: |
    version: v1
    mig-configs:
      all-disabled:
        - devices: all
          mig-enabled: false

      custom-mig:
        - devices: all  # it's possible to target single GPU's here
          mig-enabled: true
          mig-devices:
            "1g.5gb": 2
            "2g.10gb": 1
            "3g.20gb": 1</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_patch_for_clusterpolicy"><a class="anchor" href="#_patch_for_clusterpolicy"></a>4.1.2. Patch for <code>ClusterPolicy</code></h4>
<div class="ulist">
<ul>
<li>
<p>You need to modify the <code>gpu-cluster-policy</code> within the <code>nvidia-gpu-operator</code> namespace to point to your <code>custom-mig-config</code>.</p>
</li>
<li>
<p>This is typically accomplished with a Kustomize patch.</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>If the custom configuration specifies more than one instance profile, set the strategy to <code>mixed</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc patch clusterpolicies.nvidia.com/cluster-policy \
    --type='json' \
    -p='[{"op":"replace", "path":"/spec/mig/strategy", "value":"mixed"}]'</code></pre>
</div>
</div>
</li>
<li>
<p>Patch the cluster policy so MIG Manager uses the custom config map:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc patch clusterpolicies.nvidia.com/cluster-policy \
    --type='json' \
    -p='[{"op":"replace", "path":"/spec/migManager/config/name", "value":"custom-mig-config"}]'</code></pre>
</div>
</div>
</li>
<li>
<p>Label the nodes with the profile to configure:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc label nodes &lt;node-name&gt; nvidia.com/mig.config=custom-mig --overwrite</code></pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_practical_time_slicing_configuration"><a class="anchor" href="#_practical_time_slicing_configuration"></a>4.2. Practical: Time-Slicing Configuration</h3>
<div class="admonitionblock caution">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
This is the section you will actively configure for this lab.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>NVIDIA&#8217;s time slicing is a powerful feature that allows you to share a single GPU among multiple processes, where each process gets a slice of time to access the GPU&#8217;s resources.<br>
This is particularly useful for running many lightweight, concurrent workloads on a single GPU. It improves utilization and throughput without requiring multiple GPUs or a complex resource management system.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Shared GPU Resources: Multiple workloads share the same physical GPU, increasing utilization and efficiency.</p>
</li>
<li>
<p>Simpler Configuration: Compared to MIG, time slicing is easier to set up and manage, as it doesn&#8217;t require partitioning the GPU at the hardware level.</p>
</li>
<li>
<p>Best for Lightweight Workloads: Ideal for running many small AI inference tasks or other GPU-accelerated workloads that don&#8217;t saturate a full GPU.</p>
</li>
<li>
<p>Dynamic Resource Sharing: The GPU scheduler dynamically allocates GPU time to each process, ensuring fair access.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_configmap_for_time_slicing"><a class="anchor" href="#_configmap_for_time_slicing"></a>4.2.1. ConfigMap for Time Slicing</h4>
<div class="paragraph">
<p>Create a YAML file to define how you want to slice your GPUs.<br>
This <code>ConfigMap</code> can be named anything, but it must reside in the <code>nvidia-gpu-operator</code> namespace.</p>
</div>
<div class="paragraph">
<p>In this configuration, we need to define the number of replicas (slices) for each GPU model.</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">cat &lt;&lt;EOF | oc apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: device-plugin-config
  namespace: nvidia-gpu-operator
data:
  time-sliced: |-
    version: v1
    sharing:
      timeSlicing:
        resources:
          - name: nvidia.com/gpu
            replicas: 8
EOF</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_patch_for_clusterpolicy_2"><a class="anchor" href="#_patch_for_clusterpolicy_2"></a>4.2.2. Patch for ClusterPolicy</h4>
<div class="paragraph">
<p>We need to modify the <code>gpu-cluster-policy</code> within the <code>nvidia-gpu-operator</code> namespace:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>to enable GPU Feature Discovery (component of the NVIDIA GPU Operator whose primary job is to discover the hardware features of the GPUs on a node and expose them as Kubernetes node labels)</p>
</li>
<li>
<p>to point to the <code>device-plugin-config</code>.
This tells the NVIDIA Device Plugin to use the configuration you&#8217;ve defined.
Patch the ClusterPolicy so the Device Plugin uses the custom config map:</p>
</li>
</ul>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc patch clusterpolicy gpu-cluster-policy \
    -n nvidia-gpu-operator --type json \
    -p '[{"op": "replace", "path": "/spec/gfd/enabled", "value": true}]'</code></pre>
</div>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc patch clusterpolicy gpu-cluster-policy \
  -n nvidia-gpu-operator --type merge \
  -p '{"spec": {"devicePlugin": {"config": {"name": "device-plugin-config"}}}}'</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_label_the_nodes"><a class="anchor" href="#_label_the_nodes"></a>4.2.3. Label the nodes</h4>
<div class="paragraph">
<p>After patching the ClusterPolicy, you need to label the nodes that have the GPUs you want to time-slice.
The GPU Operator will automatically detect this label and apply the new configuration.</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc label --overwrite node \
    --selector=nvidia.com/gpu.product=NVIDIA-A10G-SHARED \
    nvidia.com/device-plugin.config=time-sliced</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">Label Selector for Nodes</div>
<div class="paragraph">
<p>The selector value <code>nvidia.com/gpu.product=NVIDIA-A10G-SHARED</code> must match the GPU product name as labeled by the GPU Operator&#8217;s Node Feature Discovery (NFD) component.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_verify_time_slicing_was_enabled_successfully"><a class="anchor" href="#_verify_time_slicing_was_enabled_successfully"></a>4.2.4. Verify Time Slicing was enabled successfully</h4>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get node --selector=nvidia.com/gpu.product=NVIDIA-A10G-SHARED -o json | jq '.items[0].status.capacity'</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">{
  "cpu": "8",
  "ephemeral-storage": "104266732Ki",
  "hugepages-1Gi": "0",
  "hugepages-2Mi": "0",
  "memory": "32499872Ki",
  "nvidia.com/gpu": "8",
  "pods": "250"
}</code></pre>
</div>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get node --selector=nvidia.com/gpu.product=NVIDIA-A10G-SHARED -o json \
 | jq '.items[0].metadata.labels' | grep nvidia</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">  "nvidia.com/cuda.driver-version.full": "570.148.08",
  "nvidia.com/cuda.driver-version.major": "570",
  "nvidia.com/cuda.driver-version.minor": "148",
  "nvidia.com/cuda.driver-version.revision": "08",
  "nvidia.com/cuda.driver.major": "570",
  "nvidia.com/cuda.driver.minor": "148",
  "nvidia.com/cuda.driver.rev": "08",
  "nvidia.com/cuda.runtime-version.full": "12.8",
  "nvidia.com/cuda.runtime-version.major": "12",
  "nvidia.com/cuda.runtime-version.minor": "8",
  "nvidia.com/cuda.runtime.major": "12",
  "nvidia.com/cuda.runtime.minor": "8",
  "nvidia.com/device-plugin.config": "time-sliced",
  "nvidia.com/gfd.timestamp": "1757166356",
  "nvidia.com/gpu-driver-upgrade-state": "upgrade-done",
  "nvidia.com/gpu.compute.major": "8",
  "nvidia.com/gpu.compute.minor": "6",
  "nvidia.com/gpu.count": "1",
  "nvidia.com/gpu.deploy.container-toolkit": "true",
  "nvidia.com/gpu.deploy.dcgm": "true",
  "nvidia.com/gpu.deploy.dcgm-exporter": "true",
  "nvidia.com/gpu.deploy.device-plugin": "true",
  "nvidia.com/gpu.deploy.driver": "true",
  "nvidia.com/gpu.deploy.gpu-feature-discovery": "true",
  "nvidia.com/gpu.deploy.node-status-exporter": "true",
  "nvidia.com/gpu.deploy.nvsm": "",
  "nvidia.com/gpu.deploy.operator-validator": "true",
  "nvidia.com/gpu.family": "ampere",
  "nvidia.com/gpu.machine": "g5.2xlarge",
  "nvidia.com/gpu.memory": "23028",
  "nvidia.com/gpu.mode": "compute",
  "nvidia.com/gpu.present": "true",
  "nvidia.com/gpu.product": "NVIDIA-A10G-SHARED",
  "nvidia.com/gpu.replicas": "8",
  "nvidia.com/gpu.sharing-strategy": "time-slicing",
  "nvidia.com/mig.capable": "false",
  "nvidia.com/mig.strategy": "single",
  "nvidia.com/mps.capable": "false",
  "nvidia.com/vgpu.present": "false",</code></pre>
</div>
</div>
<div class="paragraph">
<p>As expected we see the label declaring 8 replicas, as defined in our configuration.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_configure_hardware_profile_in_openshift_ai"><a class="anchor" href="#_configure_hardware_profile_in_openshift_ai"></a>5. Configure Hardware Profile in OpenShift AI</h2>
<div class="sectionbody">
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="title">Timeslicing due to hardware resource constraints</div>
<div class="paragraph">
<p>The configuration can be done even without MIG configured within the GPU Operator. But the workload will not be able to be scheduled by the OpenShift scheduler and the Pod will stay pending afterwards.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>MIG technology enables a single physical GPU to be logically partitioned into multiple, isolated GPU instances, thereby maximizing hardware utilization and facilitating multi-tenancy on expensive accelerator resources. These granular GPU configurations, along with other specialized hardware specifications, are then encapsulated within Accelerator Profiles (or the more advanced Hardware Profiles) in OpenShift AI. These profiles serve as administrative definitions that abstract complex resource configurations, allowing data scientists to easily request and consume appropriate hardware for their workbenches, model serving, and data pipelines without needing deep Kubernetes expertise.</p>
</div>
<div class="paragraph">
<p>Complementing this, Taints and Tolerations are fundamental Kubernetes primitives that ensure intelligent workload scheduling. GPU-enabled nodes can be "tainted" to prevent general workloads from being scheduled on them. Correspondingly, Accelerator/Hardware Profiles automatically apply "tolerations" to AI/ML workloads, allowing them to be scheduled exclusively on nodes possessing the required specialized hardware.</p>
</div>
<div class="sect2">
<h3 id="_create_hardware_profiles_in_rhoai"><a class="anchor" href="#_create_hardware_profiles_in_rhoai"></a>5.1. Create Hardware Profiles in RHOAI</h3>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="title">Timeslicing due to hardware resource constraints</div>
<div class="paragraph">
<p>This can be done even without MIG enabled. But the created Pods will not be able to be scheduled!</p>
</div>
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p>Hardware Profiles for each MIG Type have to be created beforehand.</p>
</li>
<li>
<p>In case Taints are configured, add the Tolerations so that the GPU-enabled pods can be immune to theme.</p>
</li>
<li>
<p>Use the resource label and display name <code>nvidia.com/mig-2g.20gb</code> inside the section <strong>Resource requests and limits</strong>.</p>
</li>
</ul>
</div>
<div class="imageblock bordershadow">
<div class="content">
<img src="_images/92-create-hardware-profile.png" alt="92 create hardware profile">
</div>
</div>
<div class="imageblock bordershadow">
<div class="content">
<img src="_images/92-resource-request-hw-profile.png" alt="92 resource request hw profile">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Click on <code>Create Hardware Profile</code></strong>.</p>
</li>
</ul>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="title">Accelerator Profiles are deprecated</div>
<div class="paragraph">
<p><code>AcceleratorProfiles</code> will be replaced by <code>HardwareProfiles</code>. They are more flexible and should be the preferred profile.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Thanks to the Cloud Native approach of RHOAI, the profile can be created as <code>yaml</code> file as well to better integrate it into a GitOps approach:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: dashboard.opendatahub.io/v1alpha1
kind: HardwareProfile
metadata:
  annotations:
    opendatahub.io/dashboard-feature-visibility: '["model-serving"]' # only visible in model serving
  name: small
  namespace: redhat-ods-applications
spec:
  description: Mig-2g.20gb to test hardware profile
  displayName: small
  enabled: true
  identifiers:
    - defaultCount: 2
      displayName: CPU
      identifier: cpu
      maxCount: 4
      minCount: 1
      resourceType: CPU
    - defaultCount: 4Gi
      displayName: Memory
      identifier: memory
      maxCount: 8Gi
      minCount: 2Gi
      resourceType: Memory
    - defaultCount: 1
      displayName: nvidia.com/mig-2g.20gb
      identifier: nvidia.com/mig-2g.20gb
      maxCount: 2
      minCount: 1
      resourceType: Accelerator
  nodeSelector: {}
  tolerations: []</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The Blog article <a href="https://developers.redhat.com/articles/2025/01/30/build-and-deploy-modelcar-container-openshift-ai?source=sso#">Build and deploy a ModelCar container in OpenShift AI</a> demonstrates how to build a ModelCar Container and discusses pros and cons about the ModelCar Approach.<br>
In the next steps we will use the ModelCar available at <code>oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-2b-instruct</code> to deploy a Model using OpenShift AI.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_verify_the_configuration"><a class="anchor" href="#_verify_the_configuration"></a>6. Verify the Configuration</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_create_models"><a class="anchor" href="#_create_models"></a>6.1. Create Models</h3>
<div class="paragraph">
<p>In this section two models will be deployed. One will use the <code>nvidia.com/gpu</code> accelerator, whike the other model will use the <code>nvidia.com/mig-2g.20gb</code> accelerator.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a new Project in OpenShift AI:</p>
<div class="imageblock">
<div class="content">
<img src="_images/92-rhoai-project-gpuaas.png" alt="92 rhoai project gpuaas">
</div>
</div>
</li>
<li>
<p>Create a <code>Connection</code> within the <code>granite</code> Project:</p>
<div class="imageblock">
<div class="content">
<img src="_images/92-create-data-connection.png" alt="92 create data connection">
</div>
</div>
</li>
<li>
<p>Deploy a Model within the <code>granite</code> project (go to <code>Models</code> &#8594; <code>Single-model serving platform</code> &#8594; <code>Deploy Model</code>), using the new HardwareProfile created beforehand <code>small</code>:</p>
<div class="admonitionblock caution">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
<div class="title">Pod will stay <code>Pending</code> forever</div>
<div class="paragraph">
<p>The Hardware Profiles can be created even when the resources are not present in the Cluster. The OpenShift scheduler will not be able to schedule the Pod!</p>
</div>
</td>
</tr>
</table>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/92-create-model-mig.png" alt="92 create model mig">
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_inspect_the_resource_requests"><a class="anchor" href="#_inspect_the_resource_requests"></a>6.2. Inspect the resource requests</h3>
<div class="paragraph">
<p>The Model <code>granite-3.3-2b-instruct</code> should work using the <code>nvidia-com/gpu</code> idientifier whereas the Model <code>granite-3.3-2b-instruct-mig</code> will stay pending.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="title">Timeslicing due to hardware resource constraints</div>
<div class="paragraph">
<p>Created resources will contain the resource <code>nvidia.com/mig-2g.20gb: "1"</code>, which is not present in the Cluster. The OpenShift scheduler will not be able to schdule the <code>pods</code>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>While inspecting the resource (which will be created RHOAI while serving a Model) the <code>spec.containers[0].resources.requests</code> will use the resource <code>nvidia.com/mig-2g.20gb</code> which is not present in the cluster.</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get pods -n granite -o yaml |grep nvidia -B 3</code></pre>
</div>
</div>
<div class="paragraph">
<p>The output will look like the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">        limits:
          cpu: "2"
          memory: 4Gi
          nvidia.com/mig-2g.20gb: "1"
        requests:
          cpu: "2"
          memory: 4Gi
          nvidia.com/mig-2g.20gb: "1"
--
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-19T14:59:18Z"
      message: '0/3 nodes are available: 1 Insufficient cpu, 1 Insufficient nvidia.com/mig-2g.20gb,
        2 node(s) had untolerated taint {nvidia.com/gpu: Exists}. preemption: 0/3</code></pre>
</div>
</div>
<div class="paragraph">
<p>As explained earlier, when applying the MIG configuratiuon within a Cluster which does not have an accelerator type (i.e. <code>nvidia.com/mig-2g.20gb</code>) the scheduler will not be able to be scheduled, therefore affected pods will stay in <code>Pending</code> state.</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc delete project granite</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_references"><a class="anchor" href="#_references"></a>References</h2>
<div class="sectionbody">
<div class="ulist bibliography">
<ul class="bibliography">
<li>
<p><a id="documentation"></a>[documentation] Red Hat. <em>OpenShift Documentation</em>. Version 4.19. Available from: <a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/hardware_accelerators/nvidia-gpu-architecture#nvidia-gpu-bare-metal_nvidia-gpu-architecture#:~:text=In%20addition%2C%20the,the%20same%20node" class="bare">https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/hardware_accelerators/nvidia-gpu-architecture#nvidia-gpu-bare-metal_nvidia-gpu-architecture#:~:text=In%20addition%2C%20the,the%20same%20node</a></p>
</li>
<li>
<p><a id="documentation"></a>[documentation] Red Hat. <em>OpenShift AI Documentation</em>. Version 2.23. Available from: <a href="https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.23/html/working_with_accelerators/working-with-hardware-profiles_accelerators" class="bare">https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.23/html/working_with_accelerators/working-with-hardware-profiles_accelerators</a></p>
</li>
</ul>
</div>
</div>
</div>
</article>
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
  </div>
</main>
</div>
<footer class="footer">
  <a href="https://demo.redhat.com" target="_blank" class="poweredBy"><p>Powered by</p><div class="labInfo_poweredBy" style="display: block; width: 260px;"><svg class="labInfo_poweredByLogo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1508.5 178.739"><g fill="#111"><path d="M316.6 63.2v-56H342a21.279 21.279 0 0 1 7.8 1.3 18.111 18.111 0 0 1 5.9 3.5 15.577 15.577 0 0 1 5 11.8 15.051 15.051 0 0 1-3.1 9.5 16.836 16.836 0 0 1-8.4 5.8l12.5 24.1h-9.3l-11.6-23H325v23Zm24.7-48.6H325v18.7h16.3q5.25 0 8.1-2.7a8.7 8.7 0 0 0 2.8-6.6 8.7 8.7 0 0 0-2.8-6.6c-1.8-1.9-4.5-2.8-8.1-2.8ZM364.1 42.8a20.674 20.674 0 0 1 1.6-8.2 20.288 20.288 0 0 1 4.3-6.7 19.92 19.92 0 0 1 6.5-4.5 19.718 19.718 0 0 1 8-1.6 18.463 18.463 0 0 1 7.8 1.6 18.677 18.677 0 0 1 6.2 4.5 20.927 20.927 0 0 1 4.1 6.8 23.2 23.2 0 0 1 1.5 8.4v2.3H372a13.822 13.822 0 0 0 4.6 8.4 13.6 13.6 0 0 0 9.1 3.3 15.553 15.553 0 0 0 5.7-1 12.858 12.858 0 0 0 4.6-2.6l5.1 5a25.983 25.983 0 0 1-7.4 4.1 24.69 24.69 0 0 1-8.4 1.3 21.306 21.306 0 0 1-8.4-1.6 22.763 22.763 0 0 1-6.8-4.4 20.788 20.788 0 0 1-4.5-6.7 23.2 23.2 0 0 1-1.5-8.4Zm20.2-14.2a11.527 11.527 0 0 0-8 3 13.046 13.046 0 0 0-4.2 7.8h24.2a13.091 13.091 0 0 0-4.2-7.8 11.106 11.106 0 0 0-7.8-3ZM443.1 63.2v-3.8a19.448 19.448 0 0 1-5.8 3.3 18.924 18.924 0 0 1-6.7 1.2 19.824 19.824 0 0 1-14.6-6.1 22.268 22.268 0 0 1-4.4-6.7 21.812 21.812 0 0 1 0-16.4A20.534 20.534 0 0 1 416 28a19.335 19.335 0 0 1 6.6-4.5 20.334 20.334 0 0 1 8.2-1.6 20.7 20.7 0 0 1 6.6 1 19.415 19.415 0 0 1 5.7 3V7.2l8-1.8v57.8Zm-25.2-20.4a13.718 13.718 0 0 0 4 10.1 13.45 13.45 0 0 0 9.8 4.1 14.956 14.956 0 0 0 6.4-1.3 15.954 15.954 0 0 0 4.9-3.6V33.6a14.988 14.988 0 0 0-4.9-3.5 15.271 15.271 0 0 0-6.4-1.3 13.423 13.423 0 0 0-9.9 4 13.806 13.806 0 0 0-3.9 10ZM478.1 63.2v-56h8.4v24h29.8v-24h8.4v56h-8.4V38.8h-29.8v24.4ZM547.2 64a16.483 16.483 0 0 1-10.8-3.5 11.037 11.037 0 0 1-4.2-8.9 10.375 10.375 0 0 1 4.7-9.2 20.76 20.76 0 0 1 11.8-3.2 27.841 27.841 0 0 1 5.8.6 27.374 27.374 0 0 1 5.3 1.6v-4.3a8.143 8.143 0 0 0-2.6-6.5 11.452 11.452 0 0 0-7.4-2.2 20.788 20.788 0 0 0-6 .9 34.616 34.616 0 0 0-6.6 2.6l-3-6a54.169 54.169 0 0 1 8.4-3.1 33.18 33.18 0 0 1 8.3-1.1c5.2 0 9.3 1.3 12.2 3.8s4.4 6.1 4.4 10.8v27h-7.8v-3.5a19.441 19.441 0 0 1-5.8 3.2 23.54 23.54 0 0 1-6.7 1Zm-7.3-12.6a5.646 5.646 0 0 0 2.6 4.8 11.193 11.193 0 0 0 6.6 1.8 16.256 16.256 0 0 0 5.9-1 14.449 14.449 0 0 0 4.9-2.9V47a19.778 19.778 0 0 0-4.8-1.8 24.933 24.933 0 0 0-5.7-.6 11.859 11.859 0 0 0-6.8 1.8 5.728 5.728 0 0 0-2.7 5ZM580.6 53.2v-24H572v-6.7h8.6V12.1l7.9-1.9v12.3h12v6.7h-12v22.1a5.94 5.94 0 0 0 1.4 4.4c.9.9 2.5 1.3 4.6 1.3a23.637 23.637 0 0 0 3-.2 10.857 10.857 0 0 0 2.8-.8v6.7a19.28 19.28 0 0 1-3.8.9 27.484 27.484 0 0 1-3.8.3c-3.9 0-7-.9-9-2.8-2-1.7-3.1-4.4-3.1-7.9Z"></path></g><path d="M127 90.2c12.5 0 30.6-2.6 30.6-17.5a12.678 12.678 0 0 0-.3-3.4L149.8 37c-1.7-7.1-3.2-10.3-15.7-16.6-9.7-5-30.8-13.1-37.1-13.1-5.8 0-7.5 7.5-14.4 7.5-6.7 0-11.6-5.6-17.9-5.6-6 0-9.9 4.1-12.9 12.5 0 0-8.4 23.7-9.5 27.2a4.216 4.216 0 0 0-.3 1.9c0 9.2 36.3 39.4 85 39.4Zm32.5-11.4c1.7 8.2 1.7 9.1 1.7 10.1 0 14-15.7 21.8-36.4 21.8-46.8 0-87.7-27.4-87.7-45.5a17.535 17.535 0 0 1 1.5-7.3C21.8 58.8 0 61.8 0 81c0 31.5 74.6 70.3 133.7 70.3 45.3 0 56.7-20.5 56.7-36.6-.1-12.8-11-27.3-30.9-35.9Z" fill="#e00"></path><path d="M159.5 78.8c1.7 8.2 1.7 9.1 1.7 10.1 0 14-15.7 21.8-36.4 21.8-46.8 0-87.7-27.4-87.7-45.5a17.535 17.535 0 0 1 1.5-7.3l3.7-9.1a4.877 4.877 0 0 0-.3 2c0 9.2 36.3 39.4 85 39.4 12.5 0 30.6-2.6 30.6-17.5a12.678 12.678 0 0 0-.3-3.4Z"></path><path d="M253.5 158.7a2.22 2.22 0 0 1-2.2-2.2V2.2a2.2 2.2 0 0 1 4.4 0v154.2a2.242 2.242 0 0 1-2.2 2.3Z" fill="#111"></path><text data-name="Demo Platform" transform="translate(1186 149)" fill="#111" font-size="82" font-family="'RedHatDisplay', 'Overpass', overpass, helvetica, arial, sans-serif" font-weight="700"><tspan x="-877.892" y="0">Demo Platform</tspan></text></svg></div></a>
</footer>
<script src="../_/js/vendor/clipboard.js"></script>
<script src="../_/js/site.js"></script>
<script async src="../_/js/vendor/highlight.js"></script>
  </body>
</html>
