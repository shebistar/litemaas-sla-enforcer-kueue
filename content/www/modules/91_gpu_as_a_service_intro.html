<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Lab Introduction: GPU as a Service with GPU slicing and Kueue :: Platform Foundation Bootcamp - RHOAI</title>
    <link rel="canonical" href="https://redhat-ai-services.github.io/rhoai-platform-foundation-bootcamp-instructions/modules/91_gpu_as_a_service_intro.html">
    <meta name="generator" content="Antora 3.1.14">
<link rel="stylesheet" href="../_/css/site.css"><link rel="stylesheet" href="../_/css/site-extra.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<meta name="robots" content="all">
<link rel="icon" href="../_/img/favicon.ico" type="image/x-icon">
    <link rel="icon" href="https://demo.redhat.com/images/favicon.ico" type="image/x-icon">
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar"  style="background-color: #131313 !important;">
    <div class="navbar-brand">
      <div style="display: flex; flex-direction:row; padding: 12px 32px; gap: 16px;">
     </div>
      <div class="navbar-item site-title" style="color: #fff !important;flex: 1;">Platform Foundation Bootcamp - RHOAI</div>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="modules" data-version="">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title" style="display: none;"><a href="index.html" class=" query-params-link">Navigation</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="01_welcome.html">Welcome and Introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="05_environment_provisioning.html">Environment Provisioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">AI-Accelerator</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="20_ai-accelerator_review.html">Project Overview</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="07_installation.html">Bootstrap Installation</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="30_gitops_env_setup_dev_prod.html">Setup Dev &amp; Prod Environments</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">RHOAI Administration</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="32_dashboard_configuration.html">RHOAI Dashboard Configuration</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Notebooks</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="31_custom_notebook.html">Custom Notebook</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="09_remote_connect_notebook.html">Connect to Workbench Kernel</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Train, Store (S3), Deploy</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="33_model_training_car.html">Model Training</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="34_using_s3_storage.html">Using S3 Storage</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="36_deploy_model.html">Deploy Model</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Data Science Pipelines</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="40_setup_pipeline_server.html">Setup Pipeline Server</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="41_introduction_to_kfp_pipelines.html">KFP Pipelines</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="kfp_elyra_differences.html">Comparison between Elyra and KFP</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="build_custom_runtime_image.html">Custom Runtime Image</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="introduction_to_elyra_pipelines.html">Elyra Pipelines</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="42_working_with_pipelines.html">Working with Pipelines</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="43_custom_runtime_image.html">Advanced Pipeline Customization</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Distributed Training</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="50_distributed_training.html">Ray Cluster</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Large Language Model [LLM]</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="60_llm_explore.html">Explore LLMs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="70_rag_llm.html">RAG with LLM</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="composer_ai.html">Composer AI</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Monitoring Data Science Models</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="80_trustyai_overview.html">TrustyAI Overview</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="81_llm_evaluation.html">Evaluating Large Language Models</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Disconnected Environment</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="disconnected_install.html">RHOAI on Disconnected Environment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">GPU as a Service</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="90_environment_provisioning.html">Provisioning a GPU Environment with NVIDIA A10G Tensor Core GPU</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="91_gpu_as_a_service_intro.html">Introduction: GPU as a Service with GPU slicing and Kueue</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="92_nvidia_gpu_operator.html">Configuring NVIDIA GPU Time-Slicing on OpenShift</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="93_kueue_setup.html">Red Hat build of Kueue Operator Setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="94_kueue_gpu_pricing_tier.html">Implementing GPU Pricing Tiers with Kueue</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="95_kueue_fair_sharing.html">Advanced GPU Quota Management and Preemption with Kueue</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Model-as-a-Service</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="100_maas_intro.html">Introduction</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="101_maas_bootstrap.html">Environment Bootstrap</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="102_maas_as_developer.html">Using MaaS as Developer</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="103_maas_as_platform_engineer.html">Configure a new model</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="104_maas_monitor.html">Monitor usage</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Agentic AI with Llama Stack</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="97_agentic_ai_llama_stack_introduction.html">Introduction &amp; Setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="98_agentic_ai_llama_stack_notebook_agents.html">Agentic AI Agents with Llama Stack Clients</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="99_agentic_ai_llama_stack_playground.html">Llama Stack Playground</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="#99_useful_tips.adoc">Useful Tips</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="#97_syncing_fork.adoc">Syncing Forked Project</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
  </div>
  <ul class="components">
    <li class="component is-current">
      <span class="title">Navigation</span>
      <ul class="versions">
        <li class="version is-current">
          <a href="index.html">default</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="01_welcome.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li>GPU as a Service</li>
    <li><a href="91_gpu_as_a_service_intro.html">Introduction: GPU as a Service with GPU slicing and Kueue</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<article class="doc">
<h1 class="page">Lab Introduction: GPU as a Service with GPU slicing and Kueue</h1>
<div id="toc" class="toc">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_model_size">1. Model Size</a>
<ul class="sectlevel2">
<li><a href="#_model_weights">1.1. Model Weights</a></li>
<li><a href="#_kv_cache_key_value_cache">1.2. KV Cache (Key-Value Cache)</a></li>
<li><a href="#_general_estimation_for_kv_cache_for_granite_3_3_2b_instruct">1.3. General Estimation for KV Cache (for granite-3.3-2b-instruct)</a></li>
</ul>
</li>
<li><a href="#_gpu_optimization">2. GPU Optimization</a>
<ul class="sectlevel2">
<li><a href="#_multi_instance_gpu">2.1. Multi-Instance GPU</a></li>
</ul>
</li>
<li><a href="#_fair_resource_sharing_using_kueue">3. Fair resource sharing using Kueue</a></li>
</ul>
</div>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>This lab explores GPU slicing and its application in workload prioritization. To fully grasp the significance of these topics, a solid understanding of workload sizing is essential. Therefore, we will begin by demonstrating the vRAM calculation required to serve an <code>ibm-granite/granite-3.3-2b-instruct</code> model. This example will underscore the importance of slicing GPUs into smaller, more efficient units.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_model_size"><a class="anchor" href="#_model_size"></a>1. Model Size</h2>
<div class="sectionbody">
<div class="paragraph">
<p>We will leverage vLLM (the engine used in the Red Hat Inference Server) and its use of <a href="https://arxiv.org/abs/2309.06180">PagedAttention</a>, a technique that optimizes GPU memory management. To maximize the return on investment (ROI) of expensive GPU hardware, it is crucial to understand the precise memory consumption of a model. The following calculation for the <code>ibm-granite/granite-3.3-2b-instruct</code> model demonstrates that an entire GPU is often unnecessary, and without slicing techniques, valuable resources would be wasted.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Model Weights:</strong> This is the memory required to load the model&#8217;s parameters into the GPU.</p>
</li>
<li>
<p><strong>KV Cache (Key-Value Cache):</strong> This is dynamic memory used to store attention keys and values for active sequences. While vLLM&#8217;s PagedAttention optimizes this, it still consumes significant memory, especially with high concurrency and long sequence lengths.</p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="_model_weights"><a class="anchor" href="#_model_weights"></a>1.1. Model Weights</h3>
<div class="paragraph">
<p>The <code>granite-3.3-2b-instruct</code> model has 2.53 billion parameters. For cleaner calculations, we will approximate this as 2.5B. The memory usage for model weights depends on the data type (precision) you load it in:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>FP16 (Half Precision) or BF16 (bfloat16):</strong> Each parameter uses 2 bytes. This is the most common and recommended precision for inference.</p>
<div class="stemblock">
<div class="content">
\[2.5 \text{B parameters} \cdot 2 \text{ bytes/parameter} = 5 \text{ GB}\]
</div>
</div>
</li>
<li>
<p><strong>INT8 (8-bit Quantization):</strong> Each parameter uses 1 byte.</p>
<div class="stemblock">
<div class="content">
\[2.5 \text{B parameters} \cdot 1 \text{ byte/parameter} = 2.5 \text{ GB}\]
</div>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>vLLM typically defaults to FP16 (or <code>bfloat16</code> if supported) for an optimal balance of speed and memory. Therefore, the model weights will consume approximately <strong>5 GB</strong>.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="title">Is the <code>bfloat16</code> same as <code>float16</code>, or this is some new number format?</div>
<details>
<summary class="title">Brain Floating Point format explanation</summary>
<div class="content">
<div class="paragraph">
<p><code>bfloat16</code> refer to the Brain Floating Point format ðŸ¤¯, a 16-bit floating-point data type used for deep learning applications.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>float16</code> (FP16) has higher precision (10-bit mantissa) but a much smaller numerical range, with only a 5-bit exponent.</p>
</li>
<li>
<p><code>bfloat16</code> offers a wider numerical range than <code>float16</code> by having the same 8-bit exponent as FP32, but with less precision (7-bit mantissa).<br>
This makes <code>bfloat16</code> more suitable for deep learning training due to its ability to handle larger values and prevent numerical instability without requiring extensive gradient scaling that <code>float16</code> often needs.</p>
</li>
</ul>
</div>
</div>
</details>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_kv_cache_key_value_cache"><a class="anchor" href="#_kv_cache_key_value_cache"></a>1.2. KV Cache (Key-Value Cache)</h3>
<div class="paragraph">
<p>The KV Cache memory usage is dynamic and depends on several factors:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>max_model_len</code> (or <code>max_context_len</code>): The maximum sequence length (input prompt + generated output) that the model will process. A longer <code>max_model_len</code> means a larger potential KV Cache.</p>
</li>
<li>
<p><strong>Number of Attention Heads and Hidden Dimensions:</strong> These are model-specific architectural parameters.</p>
</li>
<li>
<p><strong>Batch Size / Concurrent Requests:</strong> More concurrent requests mean more KV Cache entries.</p>
</li>
<li>
<p><code>gpu-memory-utilization</code>: vLLM&#8217;s parameter that controls the fraction of total GPU memory to be used for the model executor, including the KV Cache. By default, it&#8217;s 0.9 (90%).</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_general_estimation_for_kv_cache_for_granite_3_3_2b_instruct"><a class="anchor" href="#_general_estimation_for_kv_cache_for_granite_3_3_2b_instruct"></a>1.3. General Estimation for KV Cache (for granite-3.3-2b-instruct)</h3>
<div class="paragraph">
<p>While precise calculation requires knowing the exact attention head configuration and desired <code>max_model_len</code>, here&#8217;s a rough idea:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>For a <strong>2.5B</strong> model, the KV Cache can add a significant amount, often a few gigabytes, and can even exceed the model weights if you&#8217;re handling many concurrent, long sequences.</p>
</li>
<li>
<p>For example, a <strong>2.5B</strong> model processing around <code>2048</code> tokens in FP16 might need an additional <strong>~0.16 GB</strong> for the KV Cache per sequence (this is a rough estimate and depends heavily on batch size and <code>max_model_len</code>).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The most reliable source for a model&#8217;s architectural parameters is its configuration file, usually named <code>config.json</code>, found alongside the model weights on Hugging Face Hub.</p>
</div>
<div class="paragraph">
<p>For <code>ibm-granite/granite-3.3-2b-instruct</code>, you would look for its <code>config.json</code> file on its Hugging Face model page: <a href="https://huggingface.co/ibm-granite/granite-3.3-2b-instruct/tree/main">granite-3.3-2b-instruct</a>.</p>
</div>
<details>
<summary class="title">granite-3.3-2b-instruct <code>config.json</code></summary>
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">{
  "architectures": [
    "GraniteForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attention_multiplier": 0.015625,
  "bos_token_id": 0,
  "embedding_multiplier": 12.0,
  "eos_token_id": 0,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "logits_scaling": 8.0,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "granite",
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "residual_multiplier": 0.22,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "vocab_size": 49159
}</code></pre>
</div>
</div>
</div>
</details>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<details>
<summary class="title"><strong>Math behind the exact estimation of the KV Cache size</strong></summary>
<div class="content">
<div class="paragraph">
<p>Let&#8217;s break down the estimated memory usage for <code>ibm-granite/granite-3.3-2b-instruct</code>.</p>
</div>
<h4 id="_model_configuration_from_config_json" class="discrete">Model Configuration from <code>config.json</code></h4>
<div class="ulist">
<ul>
<li>
<p><strong>Model Parameters:</strong> 2.53 billion</p>
</li>
<li>
<p><strong>Hidden dimension size</strong> (\(h\)): 2048</p>
</li>
<li>
<p><strong>Number of layers</strong> (\(L\)): 40</p>
</li>
<li>
<p><strong>Number of attention heads</strong>: 32</p>
</li>
<li>
<p><strong>Number of KV heads</strong>: 8</p>
</li>
<li>
<p><strong>Maximum context length</strong>: 131,072 tokens</p>
</li>
<li>
<p><strong>KV Cache data type size</strong>: 2 bytes (for FP16/BF16)</p>
</li>
</ul>
</div>
<h4 id="_a_calculate_head_dim" class="discrete">a. Calculate <code>head_dim</code></h4>
<div class="paragraph">
<p>The dimension of each attention head is <code>hidden_size</code> / <code>num_attention_heads</code>:</p>
</div>
<div class="stemblock">
<div class="content">
\[head\_dim = \frac{h}{num_{attention\_heads}} = \frac{2048}{32} = 64\]
</div>
</div>
<h4 id="_b_calculate_kv_cache_size_per_token" class="discrete">b. Calculate KV Cache Size per Token</h4>
<div class="paragraph">
<p>The size of the cache per token across all layers is:</p>
</div>
<div class="stemblock">
<div class="content">
\[\begin{align*}
\text{KV Cache per token} &amp;= 2 \times L \times num_{kv\_attention\_heads} \times head\_dim \times kv_{data\_type\_size} \\
&amp;= 2 \times 40 \times 8 \times 64 \times 2 \text{ bytes/token} \\
&amp;= 81,920 \text{ bytes/token} \quad (\approx 0.078 \text{ MiB/token})
\end{align*}\]
</div>
</div>
<h4 id="_c_total_kv_cache_size_for_a_single_max_length_sequence" class="discrete">c. Total KV Cache Size (for a single max-length sequence)</h4>
<div class="paragraph">
<p>The total KV Cache size for one request at the maximum context length is:</p>
</div>
<div class="stemblock">
<div class="content">
\[\begin{align*}
\text{Total KV Cache Size} &amp;= \text{KV Cache per token} \times max_{context\_length} \\
&amp;= 81,920 \text{ bytes/token} \times 131,072 \text{ tokens} \\
&amp;= 10,737,418,240 \text{ bytes} \\
&amp;= 10,240 \text{ MiB} \\
&amp;= 10 \text{ GiB}
\end{align*}\]
</div>
</div>
<hr>
<div class="paragraph">
<p>This <strong>10 GiB</strong> is the maximum KV Cache memory required for a <strong>single sequence</strong> that utilizes the full 131,072 token context window.</p>
</div>
<h4 id="_total_estimated_gpu_memory_for_ibm_granitegranite_3_3_2b_instruct_on_vllm_fp16" class="discrete">Total Estimated GPU Memory for <code>ibm-granite/granite-3.3-2b-instruct</code> on vLLM (FP16)</h4>
<div class="paragraph">
<p>Combining the model weights (FP16) and a typical KV Cache for vLLM serving:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Model Weights (FP16):</strong> \(\approx 5 \text{ GB}\)</p>
</li>
<li>
<p><strong>KV Cache (max single sequence):</strong> \(\approx 10 \text{ GiB}\)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Total Minimum GPU Memory:</strong> \(5 \text{ GB}\) (weights) + \(10 \text{ GiB}\) (max KV Cache) \(\approx 15-16 \text{ GB}\)</p>
</div>
<div class="paragraph">
<p>However, this is just for one active sequence. vLLM is designed for high throughput, meaning it handles multiple concurrent requests. If you have, for example, 5 concurrent sequences each using a fraction of the <code>max_model_len</code>, the KV Cache could easily demand much more memory.</p>
</div>
<div class="paragraph">
<p>Therefore, for comfortable serving of <code>ibm-granite/granite-3.3-2b-instruct</code> on vLLM:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A GPU with <strong>16GB vRAM</strong> <strong>might</strong> just barely fit if you strictly limit concurrency and context length.</p>
</li>
<li>
<p><strong>24GB vRAM</strong> (like an RTX 3090/4090 or A100) offers much more headroom for the KV Cache to scale with concurrent requests and longer sequence lengths, making it a more suitable choice for production serving.</p>
</li>
<li>
<p>If you need to fit it on smaller GPUs (e.g., 12GB), you would need to use <a href="https://developers.redhat.com/articles/2025/08/18/optimizing-generative-ai-models-quantization#what_is_quantization_">quantization</a>. <strong>8-bit quantization</strong> would bring the model weights down to <strong>2.5 GB</strong>, and <strong>4-bit quantization</strong> would reduce them to approximately <strong>1.25 GB</strong>, leaving significantly more room for the KV Cache.</p>
</li>
</ul>
</div>
<h4 id="_understanding_the_trade_off_context_length_vs_concurrency" class="discrete">Understanding the Trade-off: Context Length vs. Concurrency</h4>
<div class="paragraph">
<p>The KV Cache size scales linearly with the sequence length. While the model supports a massive 131k token context, serving a single request at this length is memory-intensive.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Model Weights (static):</strong> Approximately 5 GB (for FP16).</p>
</li>
<li>
<p><strong>KV Cache (dynamic):</strong></p>
<div class="ulist">
<ul>
<li>
<p>At max context (131k tokens): ~10 GiB per request.</p>
</li>
<li>
<p>At a common context (e.g., 2,048 tokens): ~0.16 GB per request.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>As you can see, the KV Cache for a single max-length request is twice the size of the model weights.</p>
</div>
</div>
</details>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>A common way to calculate the needed KV Cache are calculators like the <a href="https://huggingface.co/spaces/gaunernst/kv-cache-calculator">gaunernst/kv-cache-calculator</a> from Hugging Face.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_gpu_optimization"><a class="anchor" href="#_gpu_optimization"></a>2. GPU Optimization</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Our calculation shows a vRAM requirement of at least <strong>16 GB</strong> for a single user at maximum context. If we target <strong>20 GB</strong> to support a few concurrent queries, an <strong>H100 GPU</strong> with <strong>80 GB</strong> of vRAM can easily accommodate the model. However, this leaves a significant portion of the GPU unused. To boost utilization, we can leverage the H100&#8217;s slicing capabilities, which this course will explore. To boost GPU utilization, we can leverage the <strong>H100&#8217;s</strong> slicing capabilities. The rest of this course will demonstrate how to split the GPU into Multi-Instance GPU (MIG) instances, allowing us to serve up to four models of the same size and configuration concurrently.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
See also the <a href="https://github.com/rh-aiservices-bu/gpu-partitioning-guide">GPU partitioning guide</a> developed by the <code>rh-aiservices-bu</code>.
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">NVIDIA GPU Slicing/Sharing Options</div>
<h2 id="_1_time_slicing_software_based_gpu_sharing" class="discrete">1. Time-Slicing (Software-based GPU Sharing)</h2>
<div class="paragraph">
<p>Time-slicing is a software-based technique that allows a single GPU to be shared by multiple processes or containers by dividing its processing time into small intervals. Each process gets a turn to use the GPU in a round-robin fashion.</p>
</div>
<div class="paragraph">
<p><strong>How it works:</strong>
The GPU scheduler allocates time slices to each process. At the end of a time slice, the scheduler preempts the current execution, saves its context, and switches to the next process. This allows multiple workloads to appear to run concurrently on the same physical GPU.</p>
</div>
<div class="paragraph">
<p><strong>Pros:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Cost Efficiency:</strong> Maximizes the utilization of expensive GPUs, particularly for small-to-medium-sized workloads that don&#8217;t fully utilize a GPU.</p>
</li>
<li>
<p><strong>Concurrency:</strong> Enables multiple applications or users to access the GPU simultaneously.</p>
</li>
<li>
<p><strong>Broad Compatibility:</strong> Works with almost all NVIDIA GPU architectures.</p>
</li>
<li>
<p><strong>Flexibility:</strong> Can accommodate a variety of workloads.</p>
</li>
<li>
<p><strong>Simple to Implement (in Kubernetes):</strong> Can be configured using the NVIDIA GPU operator and device plugin.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Cons:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>No Memory or Fault Isolation:</strong> A crash or misbehaving task can affect all other tasks sharing the GPU.</p>
</li>
<li>
<p><strong>Potential Latency/Overhead:</strong> Context switching introduces overhead, which can impact latency-sensitive applications.</p>
</li>
<li>
<p><strong>Resource Starvation:</strong> Without careful management, some tasks might get more GPU time than others.</p>
</li>
<li>
<p><strong>No Fixed Resource Guarantees:</strong> There&#8217;s no guarantee of a fixed amount of memory or compute resources for each "slice."</p>
</li>
</ul>
</div>
<h2 id="_2_multi_instance_gpu_mig" class="discrete">2. Multi-Instance GPU (MIG)</h2>
<div class="paragraph">
<p>MIG is a hardware-based GPU partitioning feature (NVIDIA Ampere architecture and newer) that allows a single physical GPU to be partitioned into up to seven fully isolated GPU instances, each with its own dedicated compute cores, memory, and memory bandwidth.</p>
</div>
<div class="paragraph">
<p><strong>How it works:</strong>
The physical GPU is divided into independent "MIG slices" at the hardware level. Each MIG instance acts as a fully functional, smaller GPU.</p>
</div>
<div class="paragraph">
<p><strong>Pros:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Hardware Isolation:</strong> Provides strong memory and fault isolation between instances.</p>
</li>
<li>
<p><strong>Predictable Performance:</strong> Each instance has dedicated resources, offering consistent and predictable performance.</p>
</li>
<li>
<p><strong>Optimized Resource Utilization:</strong> Efficiently shares GPU resources among multiple users and workloads with varying requirements.</p>
</li>
<li>
<p><strong>Dynamic Partitioning:</strong> Administrators can dynamically adjust the number and size of MIG instances.</p>
</li>
<li>
<p><strong>Enhanced Security:</strong> Hardware isolation prevents potential data leaks between instances.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Cons:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Hardware Requirement:</strong> Only supported on NVIDIA Ampere architecture and newer (A100, H100, etc.).</p>
</li>
<li>
<p><strong>Coarse-Grained Control:</strong> Partitioning is based on predefined MIG profiles, which might not perfectly align with every workload&#8217;s exact needs.</p>
</li>
<li>
<p><strong>Fixed Resource Allocation:</strong> Once an MIG instance is created, its resources are fixed.</p>
</li>
<li>
<p><strong>Complexity:</strong> Setup and management can be more complex than time-slicing.</p>
</li>
</ul>
</div>
<h2 id="_multi_process_service_mps" class="discrete">Multi-Process Service (MPS)</h2>
<div class="paragraph">
<p>NVIDIA MPS is a CUDA feature that allows multiple CUDA applications to run concurrently on a single GPU by consolidating multiple CUDA contexts into a single server process.</p>
</div>
<div class="paragraph">
<p><strong>How it works:</strong>
An MPS server process manages all client CUDA applications, handling the scheduling and execution of kernels from multiple clients on the GPU. This reduces context switching overhead.</p>
</div>
<div class="paragraph">
<p><strong>Pros:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Improved GPU Utilization:</strong> Allows kernels and memory copy operations from different processes to overlap.</p>
</li>
<li>
<p><strong>Reduced Overhead:</strong> Minimizes context switching compared to default time-slicing.</p>
</li>
<li>
<p><strong>Concurrent Execution:</strong> Enables multiple CUDA applications to run in parallel on the same GPU.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Cons:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>No Memory Protection/Error Isolation:</strong> Similar to time-slicing, a misbehaving client can impact others.</p>
</li>
<li>
<p><strong>Limited to CUDA Applications:</strong> Primarily designed for CUDA workloads.</p>
</li>
<li>
<p><strong>Compatibility:</strong> Combining MPS with MIG is currently not supported by the NVIDIA GPU operator.</p>
</li>
</ul>
</div>
<h2 id="_no_gpu_partitioning_default_exclusive_access" class="discrete">No GPU Partitioning (Default Exclusive Access)</h2>
<div class="paragraph">
<p>By default, Kubernetes workloads are given exclusive access to their allocated GPUs. If a pod requests one GPU, it gets the entire physical GPU.</p>
</div>
<div class="paragraph">
<p><strong>Pros:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Simplicity:</strong> Easiest to configure and manage.</p>
</li>
<li>
<p><strong>Maximum Performance for Single Workload:</strong> A single workload has dedicated access to the entire GPU.</p>
</li>
<li>
<p><strong>Full Isolation (at the GPU level):</strong> Each workload runs on its own GPU.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Cons:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Low GPU Utilization:</strong> If a workload doesn&#8217;t fully saturate the GPU, significant computational power is wasted.</p>
</li>
<li>
<p><strong>Higher Costs:</strong> Requires more GPUs to run multiple smaller workloads concurrently.</p>
</li>
<li>
<p><strong>Inefficient for Small Workloads:</strong> Not suitable for many tasks that could easily share a GPU.</p>
</li>
</ul>
</div>
<h2 id="_summary_comparison" class="discrete">Summary Comparison:</h2>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Feature</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Time-Slicing</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Multi-Instance GPU (MIG)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Multi-Process Service (MPS)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Default (Exclusive Access)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Method</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Software time sharing</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Hardware partitioning</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Software context consolidation</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Full GPU allocation</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Isolation</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">None</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Hardware-enforced</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Limited/None</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Full (dedicated GPU)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Predictable Perf.</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Low</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">High</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Medium</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">High</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>GPU Utilization</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">High</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">High</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">High</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Low (for small workloads)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Hardware Req.</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">All NVIDIA GPUs</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Ampere/Hopper+</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Most NVIDIA GPUs</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">All NVIDIA GPUs</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Use Case</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Small, non-critical workloads</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Mixed workloads needing isolation</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Concurrent CUDA apps</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Large, performance-critical workloads</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Complexity</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Medium</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">High</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Medium</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Low</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>The choice of slicing option depends heavily on the specific workloads, the GPU hardware available, and the requirements for isolation, predictability, and cost efficiency.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="title">Combining MIG and Time-Slicing</div>
<div class="paragraph">
<p>You can configure the NVIDIA GPU Operator to enable time-slicing <strong>within</strong> a MIG instance. This means that after you&#8217;ve created a MIG instance (which provides hardware isolation from other MIG instances), you can then allow multiple pods to time-slice that specific MIG instance.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_multi_instance_gpu"><a class="anchor" href="#_multi_instance_gpu"></a>2.1. Multi-Instance GPU</h3>
<div class="paragraph">
<p>NVIDIA&#8217;s Multi-Instance GPU (MIG) is a technology that allows you to partition a single physical NVIDIA data center GPU (like the A100 or H100) into multiple smaller, completely isolated, and independent GPU instances.</p>
</div>
<div class="paragraph">
<p>It&#8217;s like carving up a very powerful cake into several smaller, individual slices. Each slice can then be consumed independently without affecting the others.</p>
</div>
<div class="paragraph">
<p>The GPU cannot be split arbitrarily; there are supported MIG Profiles which differ by GPU type. For the H100, for example, a valid configuration is 3x <code>MIG 3g.40gb</code> and 1x <code>MIG 1g.20gb</code> (refer to the official <a href="https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#h100-mig-profiles">H100 MIG Profiles</a> documentation for all options). With a configuration like this, multiple models could be served in parallel, with smaller slices left over for experimentation.</p>
</div>
<div class="paragraph">
<p>At the moment, the following GPUs are supported: <code>A30</code>, <code>A100</code>, <code>H100</code>, <code>H200</code>, <code>GH200</code>, and <code>B200</code>.<br>
To change the MIG profiles, the <a href="https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/introduction.html">NVIDIA GPU Operator for OpenShift</a> <code>ClusterPolicy</code> needs to be configured.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_fair_resource_sharing_using_kueue"><a class="anchor" href="#_fair_resource_sharing_using_kueue"></a>3. Fair resource sharing using Kueue</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Building upon optimized serving runtimes and efficient MIG-sliced GPU utilization, <a href="https://kueue.sigs.k8s.io/docs/overview/">Kueue</a> addresses the remaining concerns regarding fair resource sharing and workload prioritization within the OpenShift cluster.</p>
</div>
<div class="paragraph">
<p>Here are some additional use cases leveraging Kueue&#8217;s capabilities:</p>
</div>
<div class="paragraph">
<p><strong>Use Case 1: Enforcing Fair GPU Quotas Across Teams (Preventing Resource Hogging)</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Problem:</strong> Team A, with its optimized serving runtimes, could inadvertently consume all available MIG-sliced GPU resources, leaving no capacity for Team B&#8217;s critical workloads. This leads to unfair access and potential service degradation for Team B.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Use Case 2: Prioritizing Critical Runtimes Over Experiments with Preemption</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Problem:</strong> When the cluster is under heavy load, new or scaling business-critical serving instances might get stuck waiting for resources that are currently consumed by lower-priority experimental workloads (e.g., training jobs, hyperparameter sweeps).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Use Case 3: Managing Burst Capacity for Sporadic High-Priority Workloads</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Problem:</strong> Some high-priority analytical jobs or urgent model retraining tasks might sporadically require a large burst of MIG-sliced GPU resources, temporarily exceeding a team&#8217;s typical quota. Without a mechanism to handle this, these jobs might face long delays.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Use Case 4: Supporting Different Pricing Models for GPUs</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Problem:</strong> As an infrastructure provider, customers often seek to pay less for on-demand workloads like training jobs. A "spot instance" model can be implemented, offering discounted GPU resources in exchange for the possibility of preemption. Customers can use unused GPU capacity at a lower cost, but if a higher-priority workload needs the resources, the spot job is interrupted.</p>
</li>
</ul>
</div>
</div>
</div>
</article>
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
  </div>
</main>
</div>
<footer class="footer">
  <a href="https://demo.redhat.com" target="_blank" class="poweredBy"><p>Powered by</p><div class="labInfo_poweredBy" style="display: block; width: 260px;"><svg class="labInfo_poweredByLogo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1508.5 178.739"><g fill="#111"><path d="M316.6 63.2v-56H342a21.279 21.279 0 0 1 7.8 1.3 18.111 18.111 0 0 1 5.9 3.5 15.577 15.577 0 0 1 5 11.8 15.051 15.051 0 0 1-3.1 9.5 16.836 16.836 0 0 1-8.4 5.8l12.5 24.1h-9.3l-11.6-23H325v23Zm24.7-48.6H325v18.7h16.3q5.25 0 8.1-2.7a8.7 8.7 0 0 0 2.8-6.6 8.7 8.7 0 0 0-2.8-6.6c-1.8-1.9-4.5-2.8-8.1-2.8ZM364.1 42.8a20.674 20.674 0 0 1 1.6-8.2 20.288 20.288 0 0 1 4.3-6.7 19.92 19.92 0 0 1 6.5-4.5 19.718 19.718 0 0 1 8-1.6 18.463 18.463 0 0 1 7.8 1.6 18.677 18.677 0 0 1 6.2 4.5 20.927 20.927 0 0 1 4.1 6.8 23.2 23.2 0 0 1 1.5 8.4v2.3H372a13.822 13.822 0 0 0 4.6 8.4 13.6 13.6 0 0 0 9.1 3.3 15.553 15.553 0 0 0 5.7-1 12.858 12.858 0 0 0 4.6-2.6l5.1 5a25.983 25.983 0 0 1-7.4 4.1 24.69 24.69 0 0 1-8.4 1.3 21.306 21.306 0 0 1-8.4-1.6 22.763 22.763 0 0 1-6.8-4.4 20.788 20.788 0 0 1-4.5-6.7 23.2 23.2 0 0 1-1.5-8.4Zm20.2-14.2a11.527 11.527 0 0 0-8 3 13.046 13.046 0 0 0-4.2 7.8h24.2a13.091 13.091 0 0 0-4.2-7.8 11.106 11.106 0 0 0-7.8-3ZM443.1 63.2v-3.8a19.448 19.448 0 0 1-5.8 3.3 18.924 18.924 0 0 1-6.7 1.2 19.824 19.824 0 0 1-14.6-6.1 22.268 22.268 0 0 1-4.4-6.7 21.812 21.812 0 0 1 0-16.4A20.534 20.534 0 0 1 416 28a19.335 19.335 0 0 1 6.6-4.5 20.334 20.334 0 0 1 8.2-1.6 20.7 20.7 0 0 1 6.6 1 19.415 19.415 0 0 1 5.7 3V7.2l8-1.8v57.8Zm-25.2-20.4a13.718 13.718 0 0 0 4 10.1 13.45 13.45 0 0 0 9.8 4.1 14.956 14.956 0 0 0 6.4-1.3 15.954 15.954 0 0 0 4.9-3.6V33.6a14.988 14.988 0 0 0-4.9-3.5 15.271 15.271 0 0 0-6.4-1.3 13.423 13.423 0 0 0-9.9 4 13.806 13.806 0 0 0-3.9 10ZM478.1 63.2v-56h8.4v24h29.8v-24h8.4v56h-8.4V38.8h-29.8v24.4ZM547.2 64a16.483 16.483 0 0 1-10.8-3.5 11.037 11.037 0 0 1-4.2-8.9 10.375 10.375 0 0 1 4.7-9.2 20.76 20.76 0 0 1 11.8-3.2 27.841 27.841 0 0 1 5.8.6 27.374 27.374 0 0 1 5.3 1.6v-4.3a8.143 8.143 0 0 0-2.6-6.5 11.452 11.452 0 0 0-7.4-2.2 20.788 20.788 0 0 0-6 .9 34.616 34.616 0 0 0-6.6 2.6l-3-6a54.169 54.169 0 0 1 8.4-3.1 33.18 33.18 0 0 1 8.3-1.1c5.2 0 9.3 1.3 12.2 3.8s4.4 6.1 4.4 10.8v27h-7.8v-3.5a19.441 19.441 0 0 1-5.8 3.2 23.54 23.54 0 0 1-6.7 1Zm-7.3-12.6a5.646 5.646 0 0 0 2.6 4.8 11.193 11.193 0 0 0 6.6 1.8 16.256 16.256 0 0 0 5.9-1 14.449 14.449 0 0 0 4.9-2.9V47a19.778 19.778 0 0 0-4.8-1.8 24.933 24.933 0 0 0-5.7-.6 11.859 11.859 0 0 0-6.8 1.8 5.728 5.728 0 0 0-2.7 5ZM580.6 53.2v-24H572v-6.7h8.6V12.1l7.9-1.9v12.3h12v6.7h-12v22.1a5.94 5.94 0 0 0 1.4 4.4c.9.9 2.5 1.3 4.6 1.3a23.637 23.637 0 0 0 3-.2 10.857 10.857 0 0 0 2.8-.8v6.7a19.28 19.28 0 0 1-3.8.9 27.484 27.484 0 0 1-3.8.3c-3.9 0-7-.9-9-2.8-2-1.7-3.1-4.4-3.1-7.9Z"></path></g><path d="M127 90.2c12.5 0 30.6-2.6 30.6-17.5a12.678 12.678 0 0 0-.3-3.4L149.8 37c-1.7-7.1-3.2-10.3-15.7-16.6-9.7-5-30.8-13.1-37.1-13.1-5.8 0-7.5 7.5-14.4 7.5-6.7 0-11.6-5.6-17.9-5.6-6 0-9.9 4.1-12.9 12.5 0 0-8.4 23.7-9.5 27.2a4.216 4.216 0 0 0-.3 1.9c0 9.2 36.3 39.4 85 39.4Zm32.5-11.4c1.7 8.2 1.7 9.1 1.7 10.1 0 14-15.7 21.8-36.4 21.8-46.8 0-87.7-27.4-87.7-45.5a17.535 17.535 0 0 1 1.5-7.3C21.8 58.8 0 61.8 0 81c0 31.5 74.6 70.3 133.7 70.3 45.3 0 56.7-20.5 56.7-36.6-.1-12.8-11-27.3-30.9-35.9Z" fill="#e00"></path><path d="M159.5 78.8c1.7 8.2 1.7 9.1 1.7 10.1 0 14-15.7 21.8-36.4 21.8-46.8 0-87.7-27.4-87.7-45.5a17.535 17.535 0 0 1 1.5-7.3l3.7-9.1a4.877 4.877 0 0 0-.3 2c0 9.2 36.3 39.4 85 39.4 12.5 0 30.6-2.6 30.6-17.5a12.678 12.678 0 0 0-.3-3.4Z"></path><path d="M253.5 158.7a2.22 2.22 0 0 1-2.2-2.2V2.2a2.2 2.2 0 0 1 4.4 0v154.2a2.242 2.242 0 0 1-2.2 2.3Z" fill="#111"></path><text data-name="Demo Platform" transform="translate(1186 149)" fill="#111" font-size="82" font-family="'RedHatDisplay', 'Overpass', overpass, helvetica, arial, sans-serif" font-weight="700"><tspan x="-877.892" y="0">Demo Platform</tspan></text></svg></div></a>
</footer>
<script src="../_/js/vendor/clipboard.js"></script>
<script src="../_/js/site.js"></script>
<script async src="../_/js/vendor/highlight.js"></script>
  </body>
</html>
